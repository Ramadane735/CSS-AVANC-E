{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr0sLxA2m5pNATPUJZfFSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramadane735/CSS-AVANC-E/blob/main/Checkpoint_sur_le_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LVTbpWB1kvDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94adb31b-9de3-46b6-c930-6684c22fb7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titre de l'article : Intelligence artificielle\n",
            "\n",
            "Sections extraites :\n",
            "- Introduction\n",
            "\n",
            "Nombre de liens internes trouvés : 1195\n",
            "Premier lien interne : https://fr.wikipedia.org/wiki/Manipulation_boursi%C3%A8re\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        " # Question 1: Récupère le contenu HTML d'une page Wikipédia\n",
        "\n",
        "def get_html(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        " # Question 2: Extrait le titre de l'article\n",
        "\n",
        "def get_title(soup):\n",
        "    return soup.find(\"h1\", id=\"firstHeading\").text.strip()\n",
        "\n",
        " # Question 3: Extrait le contenu de l'article en structurant le texte par en-têtes\n",
        "\n",
        "def get_article_content(soup):\n",
        "\n",
        "    content = {}\n",
        "    content_div = soup.find(\"div\", id=\"mw-content-text\")\n",
        "    current_heading = \"Introduction\"\n",
        "    content[current_heading] = \"\"\n",
        "\n",
        "    for elem in content_div.find_all(['h2', 'p']):\n",
        "        if elem.name == 'h2':\n",
        "            span = elem.find('span', class_='mw-headline')\n",
        "            if span:\n",
        "                current_heading = span.text.strip()\n",
        "                content[current_heading] = \"\"\n",
        "        elif elem.name == 'p':\n",
        "            text = elem.get_text(strip=True)\n",
        "            if text:\n",
        "                content[current_heading] += text + \"\\n\"\n",
        "\n",
        "    return content\n",
        "\n",
        " # Question 4: Collecte tous les liens internes menant à d'autres pages Wikipédia\n",
        "\n",
        "def get_internal_links(soup, base_url=\"https://fr.wikipedia.org\"):\n",
        "\n",
        "    links = set()\n",
        "    for a_tag in soup.find_all(\"a\", href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.startswith(\"/wiki/\") and not any(prefix in href for prefix in [\":\", \"#\", \"//\"]):\n",
        "            full_url = urljoin(base_url, href)\n",
        "            links.add(full_url)\n",
        "    return list(links)\n",
        "\n",
        " # Question 5: Fonction principale consolidée\n",
        "\n",
        "def extract_wikipedia_data(url):\n",
        "\n",
        "    soup = get_html(url)\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": get_title(soup),\n",
        "        \"content\": get_article_content(soup),\n",
        "        \"internal_links\": get_internal_links(soup)\n",
        "    }\n",
        "\n",
        " # Question 6: Exemple de test de la dernière fonction sur une page Wikipedia\n",
        "if __name__ == \"__main__\":\n",
        "    test_url = \"https://fr.wikipedia.org/wiki/Intelligence_artificielle\"\n",
        "    data = extract_wikipedia_data(test_url)\n",
        "\n",
        "    print(\"Titre de l'article :\", data[\"title\"])\n",
        "    print(\"\\nSections extraites :\")\n",
        "    for heading in data[\"content\"].keys():\n",
        "        print(\"-\", heading)\n",
        "    print(f\"\\nNombre de liens internes trouvés : {len(data['internal_links'])}\")\n",
        "    print(\"Premier lien interne :\", data[\"internal_links\"][0] if data[\"internal_links\"] else \"Aucun lien\")\n"
      ]
    }
  ]
}